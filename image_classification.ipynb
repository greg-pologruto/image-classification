{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d12eb23-efaf-4449-9f92-6bb769c48116",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559e8db6-5dae-4c21-93fb-cea9cc6978fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset prepared successfully!\n",
      "Classes: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
      "Training samples: 12080\n",
      "Validation samples: 3007\n",
      "Test samples: 3000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set paths\n",
    "base_dir = r\"Enter File Path"\n",
    "train_dir = os.path.join(base_dir, \"seg_train\", \"seg_train\")  \n",
    "test_dir = os.path.join(base_dir, \"seg_test\", \"seg_test\")      \n",
    "\n",
    "# processed directory structure\n",
    "processed_dir = os.path.join(base_dir, \"processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for class_name in os.listdir(train_dir):\n",
    "        os.makedirs(os.path.join(processed_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "# 3. Split the data \n",
    "for class_name in os.listdir(test_dir):\n",
    "    images = os.listdir(os.path.join(test_dir, class_name))\n",
    "    # Take only 1/4 of the test images\n",
    "    images = random.sample(images, len(images) // 4)\n",
    "    for img in images:\n",
    "        src = os.path.join(test_dir, class_name, img)\n",
    "        dst = os.path.join(processed_dir, 'test', class_name, img)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Split training set into train/val (70%/15%) using only 1/4 of original data\n",
    "for class_name in os.listdir(train_dir):\n",
    "    images = os.listdir(os.path.join(train_dir, class_name))\n",
    "    # First take 1/4 of the images\n",
    "    images = random.sample(images, len(images) // 4)\n",
    "    # Then split this reduced set into train/val\n",
    "    train_imgs, val_imgs = train_test_split(images, test_size=0.1765, random_state=42)\n",
    "    \n",
    "    # Copy training images\n",
    "    for img in train_imgs:\n",
    "        src = os.path.join(train_dir, class_name, img)\n",
    "        dst = os.path.join(processed_dir, 'train', class_name, img)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy validation images\n",
    "    for img in val_imgs:\n",
    "        src = os.path.join(train_dir, class_name, img)\n",
    "        dst = os.path.join(processed_dir, 'val', class_name, img)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# IntelDataset class\n",
    "class IntelDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.images = self._load_images()\n",
    "    \n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(self.root_dir, cls)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(cls_path, img_name)\n",
    "                    images.append((img_path, self.class_to_idx[cls]))\n",
    "        return images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Define transforms \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(150),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(150),\n",
    "        transforms.CenterCrop(150),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(150),\n",
    "        transforms.CenterCrop(150),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IntelDataset(\n",
    "    root_dir=os.path.join(processed_dir, 'train'),\n",
    "    transform=data_transforms['train']\n",
    ")\n",
    "val_dataset = IntelDataset(\n",
    "    root_dir=os.path.join(processed_dir, 'val'),\n",
    "    transform=data_transforms['val']\n",
    ")\n",
    "test_dataset = IntelDataset(\n",
    "    root_dir=os.path.join(processed_dir, 'test'),\n",
    "    transform=data_transforms['test']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Verify the dataset\n",
    "print(\"\\nDataset prepared successfully!\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820905c-b4d2-427b-9870-2dfc0cad0795",
   "metadata": {},
   "source": [
    "## 2. ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038ed0f-dcd9-46fb-9db1-d5e851e80e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64  # Increased from 32\n",
    "num_workers = 4   # Multi-process data loading\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "validate_every = 2  # Validate every 2 epochs to save time\n",
    "\n",
    "# Data Transforms (unchanged)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "train_dataset = IntelDataset(\n",
    "    root_dir=os.path.join(processed_dir, 'train'),\n",
    "    transform=data_transforms['train']\n",
    ")\n",
    "val_dataset = IntelDataset(\n",
    "    root_dir=os.path.join(processed_dir, 'val'),\n",
    "    transform=data_transforms['val']\n",
    ")\n",
    "\n",
    "# Optimized DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,  # Faster data transfer to GPU\n",
    "    persistent_workers=True  # Maintain workers between epochs\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize ResNet18 (smaller than ResNet34)\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 6)  # 6 classes\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()  # For FP16 training\n",
    "\n",
    "# Training loop with mixed precision\n",
    "def train_model():\n",
    "    best_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase (every N epochs)\n",
    "        val_acc = 0.0\n",
    "        if epoch % validate_every == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100. * correct / total\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'best_model_resnet.pth')\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "              f'Time: {epoch_time:.2f}s | '\n",
    "              f'Train Loss: {train_loss/len(train_loader):.4f} | '\n",
    "              f'Train Acc: {train_acc:.2f}% | '\n",
    "              f'Val Acc: {val_acc:.2f}%' if epoch % validate_every == 0 else '')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'Training completed in {total_time/60:.2f} minutes')\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model()\n",
    "\n",
    "# Test function remains the same\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    print(f'Test Accuracy: {100. * correct / total:.2f}%')\n",
    "\n",
    "# Load best model and evaluate\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "model.load_state_dict(torch.load('best_model_resnet.pth'))\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacce23e-9099-45ef-964d-d288cfed5ad3",
   "metadata": {},
   "source": [
    "## 3. CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb0141-371c-43a5-a404-4e841539cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## Custom CNN Architecture\n",
    "class IntelCustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(IntelCustomCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = IntelCustomCNN(num_classes=6).to(device)\n",
    "\n",
    "# Loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added weight decay\n",
    "\n",
    "\n",
    "# Training function \n",
    "def train_model(model, criterion, optimizer, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.cpu().numpy())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_dataset)\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc.cpu().numpy())\n",
    "        \n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss over epochs')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy over epochs')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the custom CNN\n",
    "model = train_model(model, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "# Evaluate on test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    test_acc = running_corrects.double() / len(test_dataset)\n",
    "    print(f'Custom CNN Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0329a6b-d3ef-4d1b-92b0-da46f2904f45",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bab82-3d27-49c7-af1f-d6a742fd0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model_metrics(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Classification Report (Precision, Recall, F1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-Class Metrics DataFrame\n",
    "    metrics = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    metrics_df = pd.DataFrame(metrics).transpose()\n",
    "    \n",
    "    # Class Distribution Analysis\n",
    "    class_dist = pd.Series(all_labels).value_counts().sort_index()\n",
    "    class_dist = class_dist.set_axis(class_names)\n",
    "    \n",
    "    print(\"\\nClass Distribution in Test Set:\")\n",
    "    print(class_dist)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Get class names from dataset\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ResNet Model Evaluation\")\n",
    "print(\"=\"*50)\n",
    "resnet_metrics = evaluate_model_metrics(model, test_loader, class_names)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Custom CNN Model Evaluation\")\n",
    "print(\"=\"*50)\n",
    "custom_cnn_metrics = evaluate_model_metrics(custom_cnn_model, test_loader, class_names)\n",
    "\n",
    "# Comparative Analysis\n",
    "def compare_models(model1_metrics, model2_metrics, model1_name=\"ResNet\", model2_name=\"Custom CNN\"):\n",
    "    comparison = pd.DataFrame({\n",
    "        f'{model1_name} Precision': model1_metrics.loc[class_names, 'precision'],\n",
    "        f'{model2_name} Precision': model2_metrics.loc[class_names, 'precision'],\n",
    "        f'{model1_name} Recall': model1_metrics.loc[class_names, 'recall'],\n",
    "        f'{model2_name} Recall': model2_metrics.loc[class_names, 'recall'],\n",
    "        f'{model1_name} F1': model1_metrics.loc[class_names, 'f1-score'],\n",
    "        f'{model2_name} F1': model2_metrics.loc[class_names, 'f1-score']\n",
    "    })\n",
    "    \n",
    "    # Add macro averages\n",
    "    comparison.loc['Macro Avg'] = {\n",
    "        f'{model1_name} Precision': model1_metrics.loc['macro avg', 'precision'],\n",
    "        f'{model2_name} Precision': model2_metrics.loc['macro avg', 'precision'],\n",
    "        f'{model1_name} Recall': model1_metrics.loc['macro avg', 'recall'],\n",
    "        f'{model2_name} Recall': model2_metrics.loc['macro avg', 'recall'],\n",
    "        f'{model1_name} F1': model1_metrics.loc['macro avg', 'f1-score'],\n",
    "        f'{model2_name} F1': model2_metrics.loc['macro avg', 'f1-score']\n",
    "    }\n",
    "    \n",
    "    # Add weighted averages\n",
    "    comparison.loc['Weighted Avg'] = {\n",
    "        f'{model1_name} Precision': model1_metrics.loc['weighted avg', 'precision'],\n",
    "        f'{model2_name} Precision': model2_metrics.loc['weighted avg', 'precision'],\n",
    "        f'{model1_name} Recall': model1_metrics.loc['weighted avg', 'recall'],\n",
    "        f'{model2_name} Recall': model2_metrics.loc['weighted avg', 'recall'],\n",
    "        f'{model1_name} F1': model1_metrics.loc['weighted avg', 'f1-score'],\n",
    "        f'{model2_name} F1': model2_metrics.loc['weighted avg', 'f1-score']\n",
    "    }\n",
    "    \n",
    "    return comparison.style.background_gradient(cmap='Blues', axis=0)\n",
    "\n",
    "# Display comparative analysis\n",
    "print(\"\\nModel Comparison:\")\n",
    "compare_models(resnet_metrics, custom_cnn_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
